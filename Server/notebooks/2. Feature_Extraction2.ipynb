{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1687258678719,
     "user": {
      "displayName": "이준구",
      "userId": "13961414274136616202"
     },
     "user_tz": -540
    },
    "id": "eU0j_dcGjgxO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\venv\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 206] 파일 이름이나 확장명이 너무 깁니다: 'C:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\venv\\\\lib\\\\site-packages\\\\torchvision'\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _errors: 메모리 리소스가 부족하여 이 명령을 처리할 수 없습니다.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdgl\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01melmo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Elmo, batch_to_ids\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meunjeon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mecab\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\allennlp\\modules\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mCustom PyTorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m`Module <https://pytorch.org/docs/master/nn.html#torch.nn.Module>`_ s\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mthat are used as components in AllenNLP `Model` s.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Attention\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbones\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Backbone\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbimpm_matching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BiMpmMatching\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconditional_random_field\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConditionalRandomField\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\allennlp\\modules\\backbones\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbones\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Backbone\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbones\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpretrained_transformer_backbone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedTransformerBackbone\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbones\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvilbert_backbone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VilbertBackbone\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\allennlp\\modules\\backbones\\pretrained_transformer_backbone.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocabulary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocabulary\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbones\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Backbone\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoken_embedders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpretrained_transformer_embedder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     PretrainedTransformerEmbedder,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m     15\u001b[0m \u001b[38;5;129m@Backbone\u001b[39m\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPretrainedTransformerBackbone\u001b[39;00m(Backbone):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\allennlp\\modules\\token_embedders\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mA `TokenEmbedder` is a `Module` that\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03membeds one-hot-encoded tokens as vectors.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoken_embedders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoken_embedder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenEmbedder\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoken_embedders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoken_embedders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoken_characters_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenCharactersEncoder\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoken_embedders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01melmo_token_embedder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ElmoTokenEmbedder\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\allennlp\\modules\\token_embedders\\embedding.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m     24\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[0;32m     27\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;129m@TokenEmbedder\u001b[39m\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEmbedding\u001b[39;00m(TokenEmbedder):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv\\lib\\site-packages\\h5py\\__init__.py:25\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# --- Library setup -----------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# When importing from the root of the unpacked tarball or git checkout,\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Python sees the \"h5py\" source directory and tries to load it, which fails.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# We tried working around this by using \"package_dir\" but that breaks Cython.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _errors\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_op\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _errors: 메모리 리소스가 부족하여 이 명령을 처리할 수 없습니다."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from eunjeon import Mecab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from dgl.data.utils import load_graphs\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab 모델 다운로드\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1687258443990,
     "user": {
      "displayName": "이준구",
      "userId": "13961414274136616202"
     },
     "user_tz": -540
    },
    "id": "QTN91gj_lvZz",
    "outputId": "1ab442e7-6d54-4354-9278-1ad472c22159"
   },
   "outputs": [],
   "source": [
    "# ELMo 모델 초기화\n",
    "options_file = \"../analysis_files/elmo/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\"\n",
    "weight_file = \"../analysis_files/elmo/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\"\n",
    "elmo = Elmo(options_file, weight_file, num_output_representations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1687258443991,
     "user": {
      "displayName": "이준구",
      "userId": "13961414274136616202"
     },
     "user_tz": -540
    },
    "id": "T9wy7SHojkkV"
   },
   "outputs": [],
   "source": [
    "# 한국어 문장 형태소 분리 함수\n",
    "def tokenize_korean_sentence(sentence):\n",
    "    tokens = mecab.morphs(sentence)\n",
    "    return tokens\n",
    "\n",
    "def preprocessing(sentences) :\n",
    "    sentences = re.sub(r'\\([^)]*\\)', '', sentences)\n",
    "    sentences = sentences.replace('.', '')\n",
    "    sentences = re.sub(r'[^가-힣\\s]', '', sentences)\n",
    "    sentences = re.sub(r'\\b(?:cm|km|etc)\\b', '', sentences) \n",
    "    return sentences\n",
    "\n",
    "# 입력 문장\n",
    "sentences = [\n",
    "    '한국어 문장을 토큰화합니다.',\n",
    "    'ELMo를 적용하기 위해 문장을 변환해야 합니다.',\n",
    "    '제대로 된거 맞는거겟지?'\n",
    "]\n",
    "\n",
    "tokenized_sentences = []\n",
    "\n",
    "# 문장 임베딩 수행\n",
    "embeddings = []\n",
    "for sentence in sentences:\n",
    "    sentence = preprocessing(sentence)\n",
    "    tokens = tokenize_korean_sentence(sentence)\n",
    "    character_ids = batch_to_ids([tokens])\n",
    "    embeddings_output = elmo(character_ids)\n",
    "    sentence_embedding = embeddings_output[\"elmo_representations\"][0][0]\n",
    "    embeddings.append(sentence_embedding)\n",
    "    \n",
    "# 임베딩 결과 출력\n",
    "# for i, embedding in enumerate(embeddings):\n",
    "#     print(f\"Sentence {i+1} embedding shape:\", embedding.shape)\n",
    "\n",
    "for i, (sentence, embedding) in enumerate(zip(sentences, embeddings)):\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    print(\"Text:\", sentence)\n",
    "    print(\"Embedding shape:\", embedding.shape)\n",
    "    print(\"Embedded words:\")\n",
    "    for word, emb in zip(tokenize_korean_sentence(sentence), embedding):\n",
    "        print(f\"{word}: {emb}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../analysis_files/files/'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "executionInfo": {
     "elapsed": 2482,
     "status": "ok",
     "timestamp": 1687258446462,
     "user": {
      "displayName": "이준구",
      "userId": "13961414274136616202"
     },
     "user_tz": -540
    },
    "id": "vow8h6E0i6-x",
    "outputId": "42dcfd56-e9b6-4c84-95de-17c1614801ed"
   },
   "outputs": [],
   "source": [
    "# 감성대화말뭉치(최종데이터)_Training.csv 파일을 pandas로 읽어옵니다.\n",
    "df = pd.read_csv(os.path.join(data_dir, '감성대화말뭉치(최종데이터)_Training.csv'), encoding='cp949')\n",
    "df = df[['사람문장1', '시스템문장1', '사람문장2','시스템문장2','사람문장3','시스템문장3','감정_대분류', '상황키워드']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 문장 형태소 분리 함수\n",
    "def tokenize_korean_sentence(sentence):\n",
    "    tokens = mecab.morphs(sentence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences) :\n",
    "    sentences = re.sub(r'\\([^)]*\\)', '', sentences)\n",
    "    sentences = sentences.replace('.', '')\n",
    "    sentences = re.sub(r'[^가-힣\\s]', '', sentences)\n",
    "    sentences = re.sub(r'\\b(?:cm|km|etc)\\b', '', sentences) \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1687258446463,
     "user": {
      "displayName": "이준구",
      "userId": "13961414274136616202"
     },
     "user_tz": -540
    },
    "id": "UUnpXvirrFFZ"
   },
   "outputs": [],
   "source": [
    "# 그래프 생성 및 노드 추가\n",
    "graphs = []\n",
    "for _, row in df.iterrows():\n",
    "    G = nx.Graph()\n",
    "    for column in ['사람문장1', '시스템문장1', '사람문장2', '시스템문장2', '사람문장3', '시스템문장3']:\n",
    "        sentence = row[column]\n",
    "        print(sentence)\n",
    "        if pd.isna(sentence):  # NaN 값 처리\n",
    "            sentence_embedding = np.zeros(1024)  # 0 벡터로 처리\n",
    "        else:\n",
    "            sentence = preprocessing(sentence)\n",
    "            # 문장을 형태소로 분리\n",
    "            tokens = tokenize_korean_sentence(sentence)\n",
    "            # 문장을 ELMo 임베딩으로 변환\n",
    "            character_ids = batch_to_ids([tokens])\n",
    "            embeddings_output = elmo(character_ids)\n",
    "            sentence_embedding = torch.mean(embeddings_output[\"elmo_representations\"][0], dim=0)\n",
    "        G.add_node(sentence, feature=sentence_embedding)\n",
    "\n",
    "    G.add_edge(row['사람문장1'], row['시스템문장1'])\n",
    "    G.add_edge(row['사람문장2'], row['시스템문장2'])\n",
    "    G.add_edge(row['사람문장3'], row['시스템문장3'])\n",
    "\n",
    "    graphs.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 시각화\n",
    "for i in range(4):  # 4개의 그래프만 표시\n",
    "    graph = graphs[i]\n",
    "    pos = nx.spring_layout(graph)  # 그래프의 노드 위치 결정\n",
    "    nx.draw(graph, pos, with_labels=True, node_color='lightblue', edge_color='gray', font_size=8)  # 그래프 그리기\n",
    "    node_labels = nx.get_node_attributes(graph, 'feature')  # 노드의 임베딩 정보 가져오기\n",
    "    for node, emb in node_labels.items():\n",
    "        x, y = pos[node]\n",
    "        plt.text(x, y, s=node, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.2'), fontsize=8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_features(sentence):\n",
    "    # TF-IDF 벡터화 객체 생성\n",
    "    tfidf_vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "    # 문장 길이\n",
    "    sentence_length = len(sentence)\n",
    "\n",
    "    # 품사 개수\n",
    "    tokens = mecab.pos(sentence)\n",
    "    pos_tags = [tag for _, tag in tokens]\n",
    "    num_pos_tags = len(pos_tags)\n",
    "\n",
    "    # 명사 추출\n",
    "    nouns = mecab.nouns(sentence)\n",
    "\n",
    "    # TF-IDF 벡터화 및 상위 3개 단어 추출\n",
    "    top_words = ['', '', '']  # 단어가 없을 경우 빈 문자열로 초기화\n",
    "    top_scores = [0.0, 0.0, 0.0]  # 단어가 없을 경우 TF-IDF 스코어를 0.0으로 초기화\n",
    "    top_word_vectors = np.zeros((3,))  # 단어가 없을 경우 0 벡터로 초기화\n",
    "\n",
    "    if nouns:\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(nouns)\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "        top_indices = np.argsort(tfidf_scores)[-3:][::-1]  # 상위 3개 단어의 인덱스 추출\n",
    "        top_words = [feature_names[index] for index in top_indices]  # 상위 3개 단어 추출\n",
    "        top_scores = [tfidf_scores[index] for index in top_indices]  # 상위 3개 단어의 TF-IDF 스코어 추출\n",
    "        top_word_vectors = [tfidf_matrix.toarray()[0][index] for index in top_indices]  # 상위 3개 단어의 벡터값 추출\n",
    "\n",
    "    return sentence_length, num_pos_tags, top_words, top_scores, top_word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 간의 연결 정보를 나타내는 엣지 정보 생성\n",
    "def create_edge_index(sentences):\n",
    "    num_sentences = len(sentences)\n",
    "    edges = []\n",
    "    for i in range(num_sentences - 1):\n",
    "        edges.append((i, i + 1))\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = []\n",
    "data_test = []\n",
    "\n",
    "# 레이블을 정수형으로 변환\n",
    "label_mapping_emotion = {'기쁨': 0, '당황': 1, '분노': 2, '불안' : 3, '상처' : 4,'슬픔' : 5}  # 감정에 해당하는 레이블과 정수 매핑\n",
    "label_mapping_situation = {'가족관계': 0, '건강': 1, '건강,죽음': 2, '대인관계' : 3, '대인관계(부부, 자녀)' : 4, '연애,결혼,출산' : 5, '재정' : 6, '재정,은퇴,노후준비' : 7, '직장, 업무 스트레스' : 8, '진로,취업,직장' : 9, '학교폭력/따돌림' : 10, '학업 및 진로' : 11}  # 상황에 해당하는 레이블과 정수 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 생성 및 노드 추가\n",
    "graphs = []\n",
    "for _, row in df.iterrows():\n",
    "    G = nx.Graph()\n",
    "    for column in ['사람문장1', '시스템문장1', '사람문장2', '시스템문장2', '사람문장3', '시스템문장3']:\n",
    "        sentence = row[column]\n",
    "        print(sentence)\n",
    "        if pd.isna(sentence):  # NaN 값 처리\n",
    "            sentence_embedding = np.zeros(1024)  # 0 벡터로 처리\n",
    "        else:\n",
    "            sentence = preprocessing(sentence)\n",
    "            # 문장을 형태소로 분리\n",
    "            tokens = tokenize_korean_sentence(sentence)\n",
    "            # 문장을 ELMo 임베딩으로 변환\n",
    "            character_ids = batch_to_ids([tokens])\n",
    "            embeddings_output = elmo(character_ids)\n",
    "            sentence_embedding = torch.mean(embeddings_output[\"elmo_representations\"][0], dim=0)\n",
    "            # 문장 특징 추출\n",
    "            sentence_length, num_pos_tags, top_words, top_scores, top_word_vectors = extract_sentence_features(sentence)\n",
    "            \n",
    "\n",
    "        G.add_node(sentence, feature=sentence_embedding, length=sentence_length, pos_tags=num_pos_tags,\n",
    "                top_words=top_words, top_scores=top_scores, top_word_vectors=top_word_vectors)\n",
    "\n",
    "        \n",
    "    # 감정 및 상황 레이블 할당\n",
    "    y_emotion = label_mapping_emotion[row['감정_대분류']]\n",
    "    y_situation = label_mapping_situation[row['상황키워드']]\n",
    "    \n",
    "    G.add_edge(row['사람문장1'], row['시스템문장1'])\n",
    "    G.add_edge(row['사람문장2'], row['시스템문장2'])\n",
    "    G.add_edge(row['사람문장3'], row['시스템문장3'])\n",
    "\n",
    "    graphs.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프를 DGL 그래프로 변환\n",
    "dgl_graphs = []\n",
    "for G in graphs:\n",
    "    dgl_graphs.append(dgl.from_networkx(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "batch_size = 32\n",
    "train_graphs, test_graphs = train_test_split(dgl_graphs, test_size=0.2, random_state=42)\n",
    "data_train = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "data_test = DataLoader(test_graphs, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = dgl.nn.GraphConv(input_dim, hidden_dim)\n",
    "        self.conv2 = dgl.nn.GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = dgl.nn.GraphConv(hidden_dim, hidden_dim)\n",
    "        self.fc_emotion = nn.Linear(hidden_dim, output_dim['emotion'])\n",
    "        self.fc_situation = nn.Linear(hidden_dim, output_dim['situation'])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = x.mean(dim=0)  # 그래프의 특성을 하나의 벡터로 요약\n",
    "        emotion_out = self.fc_emotion(x)\n",
    "        situation_out = self.fc_situation(x)\n",
    "\n",
    "        return emotion_out, situation_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 모델 초기화 및 손실 함수, 옵티마이저 설정\n",
    "input_dim = 1024\n",
    "hidden_dim = 128\n",
    "output_dim = {'emotion': len(label_mapping_emotion), 'situation': len(label_mapping_situation)}\n",
    "model = GCNModel(input_dim, hidden_dim, output_dim)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수 정의\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true_emotion = []\n",
    "    y_pred_emotion = []\n",
    "    y_true_situation = []\n",
    "    y_pred_situation = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features = batch.ndata['feature']\n",
    "            edge_index = batch.edges()[0]\n",
    "            labels_emotion = batch.ndata['y_emotion']\n",
    "            labels_situation = batch.ndata['y_situation']\n",
    "\n",
    "            outputs_emotion, outputs_situation = model(features, edge_index)\n",
    "            _, predicted_emotion = torch.max(outputs_emotion, 1)\n",
    "            _, predicted_situation = torch.max(outputs_situation, 1)\n",
    "\n",
    "            y_true_emotion.extend(labels_emotion.tolist())\n",
    "            y_pred_emotion.extend(predicted_emotion.tolist())\n",
    "            y_true_situation.extend(labels_situation.tolist())\n",
    "            y_pred_situation.extend(predicted_situation.tolist())\n",
    "\n",
    "    accuracy_emotion = accuracy_score(y_true_emotion, y_pred_emotion)\n",
    "    recall_emotion = recall_score(y_true_emotion, y_pred_emotion, average='macro')\n",
    "    f1_emotion = f1_score(y_true_emotion, y_pred_emotion, average='macro')\n",
    "    accuracy_situation = accuracy_score(y_true_situation, y_pred_situation)\n",
    "    recall_situation = recall_score(y_true_situation, y_pred_situation, average='macro')\n",
    "    f1_situation = f1_score(y_true_situation, y_pred_situation, average='macro')\n",
    "\n",
    "    return accuracy_emotion, recall_emotion, f1_emotion, accuracy_situation, recall_situation, f1_situation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch in data_train:\n",
    "        features = batch.ndata['feature']\n",
    "        edge_index = batch.edges()[0]\n",
    "        labels_emotion = batch.ndata['y_emotion']\n",
    "        labels_situation = batch.ndata['y_situation']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs_emotion, outputs_situation = model(features, edge_index)\n",
    "        loss_emotion = loss_function(outputs_emotion, labels_emotion)\n",
    "        loss_situation = loss_function(outputs_situation, labels_situation)\n",
    "        loss = loss_emotion + loss_situation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 학습 중간 평가\n",
    "    accuracy_emotion_train, recall_emotion_train, f1_emotion_train, \\\n",
    "    accuracy_situation_train, recall_situation_train, f1_situation_train = evaluate(model, data_train)\n",
    "\n",
    "    # 테스트 데이터 평가\n",
    "    accuracy_emotion_test, recall_emotion_test, f1_emotion_test, \\\n",
    "    accuracy_situation_test, recall_situation_test, f1_situation_test = evaluate(model, data_test)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"Train - Emotion Accuracy: {:.2f}%, Recall: {:.2f}, F1-score: {:.2f}\".format(\n",
    "        accuracy_emotion_train * 100, recall_emotion_train, f1_emotion_train))\n",
    "    print(\"Train - Situation Accuracy: {:.2f}%, Recall: {:.2f}, F1-score: {:.2f}\".format(\n",
    "        accuracy_situation_train * 100, recall_situation_train, f1_situation_train))\n",
    "    print(\"Test - Emotion Accuracy: {:.2f}%, Recall: {:.2f}, F1-score: {:.2f}\".format(\n",
    "        accuracy_emotion_test * 100, recall_emotion_test, f1_emotion_test))\n",
    "    print(\"Test - Situation Accuracy: {:.2f}%, Recall: {:.2f}, F1-score: {:.2f}\".format(\n",
    "        accuracy_situation_test * 100, recall_situation_test, f1_situation_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 모델\n",
    "def train(model, optimizer, data_loader, task):\n",
    "    model.train()  # Set the model to train mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in data_loader:\n",
    "        optimizer.zero_grad()  # Initialize gradients\n",
    "\n",
    "        x, edge_index, labels = data.x, data.edge_index, data.y\n",
    "        if task == 'emotion':\n",
    "            labels = data.y_emotion\n",
    "        elif task == 'situation':\n",
    "            labels = data.y_situation\n",
    "\n",
    "        out_emotion, out_situation = model(x, edge_index)  # Separate outputs for emotion and situation tasks\n",
    "\n",
    "        if task == 'emotion':\n",
    "            out = out_emotion\n",
    "        elif task == 'situation':\n",
    "            out = out_situation\n",
    "\n",
    "        # Compute the loss function\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "\n",
    "        # Backpropagation and weight updates\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "\n",
    "    return total_loss / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수\n",
    "def evaluate(model, data_loader, task):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    total_correct = 0\n",
    "    total_f1 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "\n",
    "            # 데이터 배치에서 입력과 정답을 가져옴\n",
    "            x, edge_index, labels = data.x, data.edge_index, data.y\n",
    "            if task == 'emotion':\n",
    "                labels = data.y_emotion\n",
    "                max_sentence_length  = 6\n",
    "            elif task == 'situation':\n",
    "                labels = data.y_situation\n",
    "                max_sentence_length = 12\n",
    "                            \n",
    "            labels = torch.nn.functional.pad(labels, (0, max_sentence_length - labels.shape[0]), value=-1)\n",
    "\n",
    "            # 모델의 출력 계산\n",
    "            out_emotion, out_situation = model(x, edge_index)\n",
    "\n",
    "            if task == 'emotion':\n",
    "                out = out_emotion\n",
    "            elif task == 'situation':\n",
    "                out = out_situation\n",
    "            \n",
    "            # 정확도 계산\n",
    "            _, pred = torch.max(out.unsqueeze(1), dim=1)\n",
    "            correct = pred.eq(labels).sum().item()\n",
    "            total_correct += correct\n",
    "\n",
    "            # F1 점수 계산\n",
    "            f1 = f1_score(labels.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
    "\n",
    "            total_f1 += f1\n",
    "\n",
    "    accuracy = total_correct / len(data_loader.dataset)\n",
    "    f1_result = total_f1 / len(data_loader.dataset)\n",
    "\n",
    "    return accuracy, f1_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 간의 관계를 나타내는 그래프 데이터 생성\n",
    "data_list = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    sentences = [row['사람문장1'], row['시스템문장1'], row['사람문장2'], row['시스템문장2'], row['사람문장3'], row['시스템문장3']]\n",
    "    sentences = [sentence for sentence in sentences if pd.notnull(sentence)]  # Remove NaN sentences\n",
    "    sentences = preprocessing(sentences)\n",
    "\n",
    "    # 문장 특징 추출\n",
    "    sentence_lengths, pos_counts, tfidf_features = extract_sentence_features(sentences)\n",
    "\n",
    "    # 문장 간의 연결 정보를 나타내는 엣지 정보 생성\n",
    "    edges = create_edge_index(sentences)\n",
    "\n",
    "    # 문장들을 노드로 가지는 그래프 생성\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # 그래프 노드에 문장 특징 추가\n",
    "    for j, sentence in enumerate(sentences):\n",
    "        node = {\n",
    "            'sentence': sentence,\n",
    "            'sentence_length': sentence_lengths[j],\n",
    "            'pos_count': pos_counts[j],\n",
    "            'tfidf_features': tfidf_features[j]\n",
    "        }\n",
    "        graph.add_node(j, **node )\n",
    "\n",
    "    graph.add_edges_from(edges)\n",
    "    \n",
    "    adj_matrix = nx.adjacency_matrix(graph)  # 그래프의 인접 행렬을 얻습니다.\n",
    "    adj_matrix = adj_matrix.todense()  # 행렬을 밀집 행렬로 변환합니다.\n",
    "    adj_matrix = torch.FloatTensor(adj_matrix)\n",
    "    \n",
    "    # 감정 및 상황 레이블 할당\n",
    "    y_emotion = label_mapping_emotion[row['감정_대분류']]\n",
    "    y_situation = label_mapping_situation[row['상황키워드']]\n",
    "    \n",
    "    # 그래프 데이터에 감정 및 상황 레이블 추가\n",
    "    data = Data(adj=adj_matrix)  # 데이터 객체 생성\n",
    "    data.adj = sp.coo_matrix(adj_matrix)  # 인접 행렬을 SciPy 희소 행렬로 변환하여 데이터에 추가합니다.\n",
    "    data.y_emotion = torch.tensor(y_emotion)\n",
    "    data.y_situation = torch.tensor(y_situation)\n",
    "    data_list.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 137918,
     "status": "ok",
     "timestamp": 1687258584378,
     "user": {
      "displayName": "이준구",
      "userId": "13961414274136616202"
     },
     "user_tz": -540
    },
    "id": "H1CAcPmsi6-x"
   },
   "outputs": [],
   "source": [
    "# 문장 간의 관계를 나타내는 그래프 데이터 생성\n",
    "data_list = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    sentences = [row['사람문장1'], row['시스템문장1'], row['사람문장2'], row['시스템문장2'], row['사람문장3'], row['시스템문장3']]\n",
    "    sentences = [sentence for sentence in sentences if pd.notnull(sentence)]  # Remove NaN sentences\n",
    "    sentences = preprocessing(sentences)\n",
    "\n",
    "    # 문장 특징 추출\n",
    "    sentence_lengths, pos_counts, tfidf_features = extract_sentence_features(sentences)\n",
    "\n",
    "    # 문장 간의 연결 정보를 나타내는 엣지 정보 생성\n",
    "    edges = create_edge_index(sentences)\n",
    "\n",
    "    # 문장들을 노드로 가지는 그래프 생성\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # 그래프 노드에 문장 특징 추가\n",
    "    for j, sentence in enumerate(sentences):\n",
    "        node = {\n",
    "            'sentence': sentence,\n",
    "            'sentence_length': sentence_lengths[j],\n",
    "            'pos_count': pos_counts[j],\n",
    "            'tfidf_features': tfidf_features[j]\n",
    "        }\n",
    "        graph.add_node(j, **node )\n",
    "\n",
    "    graph.add_edges_from(edges)\n",
    "    \n",
    "    adj_matrix = nx.adjacency_matrix(graph)  # 그래프의 인접 행렬을 얻습니다.\n",
    "    adj_matrix = adj_matrix.todense()  # 행렬을 밀집 행렬로 변환합니다.\n",
    "    adj_matrix = torch.FloatTensor(adj_matrix)\n",
    "    \n",
    "    # 감정 및 상황 레이블 할당\n",
    "    y_emotion = label_mapping_emotion[row['감정_대분류']]\n",
    "    y_situation = label_mapping_situation[row['상황키워드']]\n",
    "    \n",
    "    # 그래프 데이터에 감정 및 상황 레이블 추가\n",
    "    data = Data(adj=adj_matrix)  # 데이터 객체 생성\n",
    "    data.adj = sp.coo_matrix(adj_matrix)  # 인접 행렬을 SciPy 희소 행렬로 변환하여 데이터에 추가합니다.\n",
    "    data.y_emotion = torch.tensor(y_emotion)\n",
    "    data.y_situation = torch.tensor(y_situation)\n",
    "    data_list.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(data_list):\n",
    "    print(f\"Graph {i+1}:\")\n",
    "    adj_matrix = data.adj.toarray()  # Convert the sparse adjacency matrix to a dense numpy array\n",
    "    graph = nx.from_numpy_array(adj_matrix)  # Create a NetworkX graph from the adjacency matrix\n",
    "    \n",
    "    # Print node values\n",
    "    for node in graph.nodes:\n",
    "        node_attrs = graph.nodes[node]\n",
    "        print(f\"Node {node}: {node_attrs}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graphs(data_list, num_graphs_to_visualize=5):\n",
    "    num_graphs = 0\n",
    "\n",
    "    for data in data_list:\n",
    "        # 그래프 생성\n",
    "        graph = nx.Graph()\n",
    "        graph.add_nodes_from(range(data.adj.shape[0]))\n",
    "        graph.add_edges_from(zip(data.adj.row, data.adj.col))\n",
    "\n",
    "        # 그래프 시각화\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        pos = nx.spring_layout(graph)  # 그래프 레이아웃 설정\n",
    "        nx.draw(graph, pos, with_labels=False, node_size=500, font_size=10)  # 그래프 시각화\n",
    "\n",
    "        # 노드 정보 표시\n",
    "        node_labels = {}\n",
    "        for node in graph.nodes:\n",
    "            node_labels[node] = f\"Node {node}\\n\"\n",
    "\n",
    "            if 'sentence_length' in graph.nodes[node]:\n",
    "                node_labels[node] += f\"Sentence Length: {graph.nodes[node]['sentence_length']}\\n\"\n",
    "\n",
    "            if 'pos_count' in graph.nodes[node]:\n",
    "                node_labels[node] += f\"POS Count: {graph.nodes[node]['pos_count']}\\n\"\n",
    "\n",
    "            if 'tfidf_features' in graph.nodes[node]:\n",
    "                node_labels[node] += f\"TF-IDF Features: {graph.nodes[node]['tfidf_features']}\\n\"\n",
    "\n",
    "            node_labels[node] += f\"y_emotion: {data.y_emotion}\\n\"\n",
    "            node_labels[node] += f\"y_situation: {data.y_situation}\"\n",
    "\n",
    "        nx.draw_networkx_labels(graph, pos, labels=node_labels, font_size=8, verticalalignment='center')  # 노드 정보 표시\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        num_graphs += 1\n",
    "        if num_graphs >= num_graphs_to_visualize:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_graphs2(data_list, num_graphs_to_visualize=5):\n",
    "    for i, data in enumerate(data_list):\n",
    "        if i >= num_graphs_to_visualize:\n",
    "            break\n",
    "        \n",
    "        adj_matrix = data.adj.toarray()  # Convert adjacency matrix to a dense matrix\n",
    "        graph = nx.from_numpy_array(adj_matrix)  # Create a NetworkX graph from the adjacency matrix\n",
    "        \n",
    "        # Retrieve node attributes\n",
    "        sentence_lengths = [graph.nodes[n]['sentence_length'] for n in graph.nodes]\n",
    "        pos_counts = [graph.nodes[n]['pos_count'] for n in graph.nodes]\n",
    "        tfidf_features = [graph.nodes[n]['tfidf_features'] for n in graph.nodes]\n",
    "        \n",
    "        # Retrieve emotion and situation labels\n",
    "        y_emotion = data.y_emotion.item()\n",
    "        y_situation = data.y_situation.item()\n",
    "        \n",
    "        # Visualize the graph\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.title(f\"Graph {i+1} (Emotion: {y_emotion}, Situation: {y_situation})\")\n",
    "        nx.draw(graph, with_labels=True, node_size=500, node_color='lightblue', font_size=10)\n",
    "        \n",
    "        # Add node attributes as labels\n",
    "        for j, node in enumerate(graph.nodes):\n",
    "            label = f\"Length: {sentence_lengths[j]}\\nPOS Count: {pos_counts[j]}\\nTF-IDF: {tfidf_features[j]}\"\n",
    "            plt.annotate(label, xy=(0, 0), xytext=(graph.nodes[node]['pos'][0], graph.nodes[node]['pos'][1] - 0.1),\n",
    "                         textcoords='axes fraction', fontsize=8, ha='center', va='center')\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "# 그래프 데이터 일부 시각화 (예: 3개 그래프)\n",
    "visualize_graphs2(data_list, num_graphs_to_visualize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "data_train, data_test = train_test_split(data_list, test_size=0.2, random_state=42)\n",
    "data_train = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "data_test = DataLoader(data_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_train))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1687258588270,
     "user": {
      "displayName": "이준구",
      "userId": "13961414274136616202"
     },
     "user_tz": -540
    },
    "id": "eFc9zIMaU_pR",
    "outputId": "2b6e4c1a-9814-4f54-f65d-7444d959f154"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc_emotion = nn.Linear(hidden_dim, output_dim['emotion'])\n",
    "        self.fc_situation = nn.Linear(hidden_dim, output_dim['situation'])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = x.mean(dim=0)  # 그래프의 특성을 하나의 벡터로 요약\n",
    "        emotion_out = self.fc_emotion(x)\n",
    "        situation_out = self.fc_situation(x)\n",
    "\n",
    "        return emotion_out, situation_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1687258588270,
     "user": {
      "displayName": "이준구",
      "userId": "13961414274136616202"
     },
     "user_tz": -540
    },
    "id": "AwmOyt6Fm5aA"
   },
   "outputs": [],
   "source": [
    "# class GNNModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(GNNModel, self).__init__()\n",
    "#         self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
    "#         self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "#         self.fc_emotion = nn.Linear(hidden_dim, output_dim['emotion'])\n",
    "#         self.fc_situation = nn.Linear(hidden_dim, output_dim['situation'])\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = x.mean(dim=0)  # 그래프의 특성을 하나의 벡터로 요약\n",
    "#         emotion_out = self.fc_emotion(x)\n",
    "#         situation_out = self.fc_situation(x)\n",
    "\n",
    "#         return emotion_out, situation_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1687258588270,
     "user": {
      "displayName": "이준구",
      "userId": "13961414274136616202"
     },
     "user_tz": -540
    },
    "id": "DJ9D2YYMm3h4"
   },
   "outputs": [],
   "source": [
    "# 학습 모델\n",
    "def train(model, optimizer, data_loader, task):\n",
    "    model.train()  # Set the model to train mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in data_loader:\n",
    "        optimizer.zero_grad()  # Initialize gradients\n",
    "\n",
    "        x, edge_index, labels = data.x, data.edge_index, data.y\n",
    "        if task == 'emotion':\n",
    "            labels = data.y_emotion\n",
    "        elif task == 'situation':\n",
    "            labels = data.y_situation\n",
    "\n",
    "        out_emotion, out_situation = model(x, edge_index)  # Separate outputs for emotion and situation tasks\n",
    "\n",
    "        if task == 'emotion':\n",
    "            out = out_emotion\n",
    "        elif task == 'situation':\n",
    "            out = out_situation\n",
    "\n",
    "        # Compute the loss function\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "\n",
    "        # Backpropagation and weight updates\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "\n",
    "    return total_loss / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1687261482871,
     "user": {
      "displayName": "이준구",
      "userId": "13961414274136616202"
     },
     "user_tz": -540
    },
    "id": "jC75g2dVm-ov"
   },
   "outputs": [],
   "source": [
    "# 평가 함수\n",
    "def evaluate(model, data_loader, task):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    total_correct = 0\n",
    "    total_f1 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "\n",
    "            # 데이터 배치에서 입력과 정답을 가져옴\n",
    "            x, edge_index, labels = data.x, data.edge_index, data.y\n",
    "            if task == 'emotion':\n",
    "                labels = data.y_emotion\n",
    "                max_sentence_length  = 6\n",
    "            elif task == 'situation':\n",
    "                labels = data.y_situation\n",
    "                max_sentence_length = 12\n",
    "                            \n",
    "            labels = torch.nn.functional.pad(labels, (0, max_sentence_length - labels.shape[0]), value=-1)\n",
    "\n",
    "            # 모델의 출력 계산\n",
    "            out_emotion, out_situation = model(x, edge_index)\n",
    "\n",
    "            if task == 'emotion':\n",
    "                out = out_emotion\n",
    "            elif task == 'situation':\n",
    "                out = out_situation\n",
    "            \n",
    "            # 정확도 계산\n",
    "            _, pred = torch.max(out.unsqueeze(1), dim=1)\n",
    "            correct = pred.eq(labels).sum().item()\n",
    "            total_correct += correct\n",
    "\n",
    "            # F1 점수 계산\n",
    "            f1 = f1_score(labels.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
    "\n",
    "            total_f1 += f1\n",
    "\n",
    "    accuracy = total_correct / len(data_loader.dataset)\n",
    "    f1_result = total_f1 / len(data_loader.dataset)\n",
    "\n",
    "    return accuracy, f1_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1687261448739,
     "user": {
      "displayName": "이준구",
      "userId": "13961414274136616202"
     },
     "user_tz": -540
    },
    "id": "F_nkHSV37Hv1"
   },
   "outputs": [],
   "source": [
    "# 데이터 로더 설정\n",
    "emotion_batch_size = 6\n",
    "situation_batch_size = 12\n",
    "\n",
    "emotion_train_dataset = data_train\n",
    "situation_train_dataset = data_train\n",
    "emotion_test_dataset = data_test\n",
    "situation_test_dataset = data_test\n",
    "\n",
    "emotion_train_loader = DataLoader(emotion_train_dataset, batch_size=emotion_batch_size, shuffle=True)\n",
    "situation_train_loader = DataLoader(situation_train_dataset, batch_size=situation_batch_size, shuffle=True)\n",
    "emotion_test_loader = DataLoader(emotion_test_dataset, batch_size=emotion_batch_size, shuffle=False)\n",
    "situation_test_loader = DataLoader(situation_test_dataset, batch_size=situation_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-1v9PNfm_Ob"
   },
   "outputs": [],
   "source": [
    "# 모델 훈련\n",
    "num_epochs = 1\n",
    "\n",
    "# 모델 초기화\n",
    "input_dim = 102  # 입력 특성의 차원\n",
    "hidden_dim = 32  # 은닉 상태의 차원\n",
    "output_dim = {'emotion': 6, 'situation': 12}  # 출력의 차원 (감정: 6개 클래스, 상황: 12개 클래스)\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "weight_decays = [1e-4, 5e-4, 1e-3]\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_f1 = 0.0\n",
    "best_learning_rate = None\n",
    "best_weight_decay = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        # 모델 초기화\n",
    "        model = GNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "        # 최적화 설정\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        # 모델 훈련\n",
    "        for epoch in range(num_epochs):\n",
    "            # 감정 학습\n",
    "            model.train()\n",
    "            emotion_train_loss = train(model, optimizer, emotion_train_loader, task='emotion')\n",
    "\n",
    "            # 감정 평가\n",
    "            emotion_acc, emotion_f1 = evaluate(model, emotion_test_loader, task='emotion')\n",
    "\n",
    "            # 상황 학습\n",
    "            model.train()\n",
    "            situation_train_loss = train(model, optimizer, situation_train_loader, task='situation')\n",
    "\n",
    "            # 상황 평가\n",
    "            situation_acc, situation_f1 = evaluate(model, situation_test_loader, task='situation')\n",
    "\n",
    "            # 결과 출력\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            print(f\"Emotion - Train Loss: {emotion_train_loss:.4f}, Acc: {emotion_acc:.4f}, F1: {emotion_f1:.4f}\")\n",
    "            print(f\"Situation - Train Loss: {situation_train_loss:.4f}, Acc: {situation_acc:.4f}, F1: {situation_f1:.4f}\")\n",
    "            print(\"--------------------------------------------------\")\n",
    "\n",
    "        # 최고 성능인 경우 기록\n",
    "        if emotion_acc > best_accuracy:\n",
    "            best_accuracy = emotion_acc\n",
    "            best_f1 = emotion_f1\n",
    "            best_learning_rate = lr\n",
    "            best_weight_decay = wd\n",
    "\n",
    "print(\"Grid Search Results:\")\n",
    "print(f\"Best Learning Rate: {best_learning_rate}\")\n",
    "print(f\"Best Weight Decay: {best_weight_decay}\")\n",
    "print(f\"Best Emotion Accuracy: {best_accuracy}\")\n",
    "print(f\"Best Emotion F1 Score: {best_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1687258588681,
     "user": {
      "displayName": "이준구",
      "userId": "13961414274136616202"
     },
     "user_tz": -540
    },
    "id": "ZENIxyxGZTks"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
